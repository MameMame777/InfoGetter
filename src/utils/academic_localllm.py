"""
Academic LocalLLM System - Specialized for Academic Paper Summarization
====================================================================

This system provides Japanese summarization of academic papers using
local LLM models, specifically optimized for technical content.
"""

import logging
import json
import os
from typing import Dict, Any, List, Optional
from pathlib import Path
import re

# For LLM integration
try:
    from llama_cpp import Llama
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    Llama = None

# For Japanese text processing
import unicodedata


class AcademicLocalLLM:
    """
    Academic paper summarization system using local Llama models
    Specialized for Japanese output of technical content
    """
    
    def __init__(self, model_path: Optional[str] = None):
        self.logger = logging.getLogger("AcademicLocalLLM")
        self.model = None
        self.model_path = model_path
        self.is_initialized = False
        
        # Academic summarization configuration
        self.config = {
            "max_tokens": 2048,
            "temperature": 0.3,  # Lower temperature for more consistent academic summaries
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "n_ctx": 4096,  # Context window
            "n_threads": 4,
            "verbose": False
        }
        
        # Initialize the model
        self._initialize_model()
    
    def _initialize_model(self) -> None:
        """Initialize the Llama model for academic summarization"""
        try:
            if not LLAMA_AVAILABLE:
                self.logger.error("âŒ llama-cpp-python not available. Install with: pip install llama-cpp-python")
                return
            
            # Find model file
            model_path = self._find_model_file()
            if not model_path:
                self.logger.warning("âš ï¸ No Llama model found. Academic summarization will use enhanced template mode.")
                return
            
            self.logger.info(f"ðŸš€ Loading Llama model for academic summarization: {model_path}")
            
            # Initialize Llama model with academic-optimized settings
            self.model = Llama(
                model_path=str(model_path),
                n_ctx=self.config["n_ctx"],
                n_threads=self.config["n_threads"],
                verbose=self.config["verbose"]
            )
            
            self.model_path = model_path
            self.is_initialized = True
            self.logger.info("âœ… Academic LocalLLM initialized successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Llama model: {e}")
            self.logger.info("ðŸ“ Falling back to enhanced academic template mode")
    
    def _find_model_file(self) -> Optional[Path]:
        """Find available Llama model file"""
        # Search paths for model files
        search_paths = [
            "models/llama-2-7b-chat.gguf",
            "models/llama-2-13b-chat.gguf", 
            "models/llama-2-7b.gguf",
            "models/CodeLlama-7b-Instruct.gguf",
            "models/mistral-7b-instruct.gguf",
            "../models/llama-2-7b-chat.gguf",
            "models/*.gguf"
        ]
        
        for pattern in search_paths:
            if "*" in pattern:
                # Glob pattern
                matches = list(Path(".").glob(pattern))
                if matches:
                    return matches[0]
            else:
                path = Path(pattern)
                if path.exists():
                    return path
        
        return None
    
    def summarize_academic_papers(self, papers_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Summarize academic papers with focus on technical innovations
        
        Args:
            papers_data: List of paper data dictionaries
            
        Returns:
            Summary result with technical focus
        """
        try:
            self.logger.info(f"ðŸ“š Processing {len(papers_data)} academic papers")
            
            # Extract and prepare academic content
            academic_content = self._prepare_academic_content(papers_data)
            
            # Generate technical summary
            if self.is_initialized and self.model:
                summary = self._generate_llm_summary(academic_content)
                processing_method = "llama-academic"
            else:
                summary = self._generate_enhanced_template_summary(academic_content)
                processing_method = "enhanced-template-academic"
            
            # Post-process for academic formatting
            formatted_summary = self._format_academic_summary(summary)
            
            return {
                "summary": formatted_summary,
                "technical_highlights": self._extract_technical_highlights(papers_data),
                "innovation_analysis": self._analyze_innovations(papers_data),
                "processing_method": processing_method,
                "paper_count": len(papers_data),
                "language": "ja",
                "model_info": {
                    "model_path": str(self.model_path) if self.model_path else "template-mode",
                    "llm_active": self.is_initialized,
                    "specialization": "academic-japanese"
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Academic summarization failed: {e}")
            raise RuntimeError(f"Academic summarization error: {e}")
    
    def _prepare_academic_content(self, papers_data: List[Dict[str, Any]]) -> str:
        """Prepare academic content for summarization"""
        content_parts = []
        
        for i, paper in enumerate(papers_data, 1):
            title = paper.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜Ž')
            abstract = paper.get('abstract', '')
            content = paper.get('content', '')
            authors = paper.get('authors', [])
            
            # Format each paper
            paper_text = f"""
è«–æ–‡ {i}: {title}
è‘—è€…: {', '.join(authors) if authors else 'ä¸æ˜Ž'}
è¦æ—¨: {abstract}
å†…å®¹: {content}
---
"""
            content_parts.append(paper_text)
        
        return "\n".join(content_parts)
    
    def _generate_llm_summary(self, content: str) -> str:
        """Generate summary using Llama LLM"""
        # Academic-focused prompt in Japanese
        prompt = f"""ä»¥ä¸‹ã®å­¦è¡“è«–æ–‡ã‚’è©³ç´°ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«æŠ€è¡“çš„ãªç‰¹å¾´ã€æ–°è¦æ€§ã€é©æ–°æ€§ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸæ—¥æœ¬èªžã§ã®è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

å­¦è¡“è«–æ–‡å†…å®¹:
{content}

è¦ç´„ã®æŒ‡é‡:
1. å„è«–æ–‡ã®ä¸»è¦ãªæŠ€è¡“çš„è²¢çŒ®ã‚’æ˜Žç¢ºã«è¨˜è¿°
2. æ–°è¦æ€§ã‚„é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å¼·èª¿
3. å®Ÿç”¨çš„ãªå¿œç”¨å¯èƒ½æ€§ã‚’è¨€åŠ
4. æŠ€è¡“çš„ãªè©³ç´°ã‚’å«ã‚€åŒ…æ‹¬çš„ãªè¦ç´„
5. æ—¥æœ¬èªžã§ã®è‡ªç„¶ãªè¡¨ç¾

è©³ç´°è¦ç´„:"""

        try:
            # Generate with academic-optimized parameters
            response = self.model(
                prompt,
                max_tokens=self.config["max_tokens"],
                temperature=self.config["temperature"],
                top_p=self.config["top_p"],
                repeat_penalty=self.config["repeat_penalty"],
                stop=["---", "\n\n\n"]
            )
            
            return response['choices'][0]['text'].strip()
            
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            return self._generate_enhanced_template_summary(content)
    
    def _generate_enhanced_template_summary(self, content: str) -> str:
        """Generate enhanced template-based academic summary"""
        self.logger.info("ðŸ“ Using enhanced academic template mode")
        
        # Extract key information using pattern matching
        technical_terms = self._extract_technical_terms(content)
        innovations = self._extract_innovation_keywords(content)
        methods = self._extract_methodology_keywords(content)
        
        # Generate structured academic summary
        summary_parts = [
            "## å­¦è¡“è«–æ–‡è¦ç´„ - æŠ€è¡“çš„ç‰¹å¾´ã¨æ–°è¦æ€§åˆ†æž\n",
            f"**å‡¦ç†è«–æ–‡æ•°**: {content.count('è«–æ–‡')}ä»¶\n",
            f"**ä¸»è¦æŠ€è¡“åˆ†é‡Ž**: {', '.join(technical_terms[:5])}\n",
            f"**é©æ–°çš„æ‰‹æ³•**: {', '.join(innovations[:3])}\n",
            f"**ç ”ç©¶æ‰‹æ³•**: {', '.join(methods[:3])}\n\n",
            "### æŠ€è¡“çš„è²¢çŒ®ã®è©³ç´°åˆ†æž:\n"
        ]
        
        # Analyze each paper section
        papers = content.split('---')
        for i, paper_section in enumerate(papers, 1):
            if paper_section.strip():
                paper_analysis = self._analyze_paper_section(paper_section)
                summary_parts.append(f"\n**è«–æ–‡{i}ã®æŠ€è¡“çš„ç‰¹å¾´**:\n{paper_analysis}\n")
        
        # Add innovation analysis
        summary_parts.extend([
            "\n### æ–°è¦æ€§ãƒ»é©æ–°æ€§ã®è©•ä¾¡:\n",
            self._generate_innovation_assessment(content),
            "\n### å®Ÿç”¨åŒ–å¯èƒ½æ€§:\n",
            self._generate_practical_assessment(content)
        ])
        
        return "".join(summary_parts)
    
    def _extract_technical_terms(self, content: str) -> List[str]:
        """Extract technical terms from content"""
        # Technical term patterns for various fields
        technical_patterns = [
            r'(?i)\b(?:FPGA|GPU|CPU|AI|ML|deep learning|neural network|algorithm|optimization|performance|efficiency|throughput|latency|bandwidth|acceleration|parallel|distributed|cloud|edge|IoT|blockchain|quantum|security|encryption|compression|processing|computing|architecture|framework|protocol|interface|implementation|methodology|analysis|evaluation|benchmark|comparison|improvement|enhancement|innovation|novel|advanced|state-of-the-art)\b',
            r'(?i)\b(?:æ©Ÿæ¢°å­¦ç¿’|æ·±å±¤å­¦ç¿’|äººå·¥çŸ¥èƒ½|ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯|ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ |æœ€é©åŒ–|æ€§èƒ½|åŠ¹çŽ‡|ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ|ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·|å¸¯åŸŸå¹…|é«˜é€ŸåŒ–|ä¸¦åˆ—|åˆ†æ•£|ã‚¯ãƒ©ã‚¦ãƒ‰|ã‚¨ãƒƒã‚¸|æš—å·åŒ–|åœ§ç¸®|å‡¦ç†|è¨ˆç®—|ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£|ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯|ãƒ—ãƒ­ãƒˆã‚³ãƒ«|ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹|å®Ÿè£…|æ‰‹æ³•|åˆ†æž|è©•ä¾¡|ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯|æ¯”è¼ƒ|æ”¹å–„|æ‹¡å¼µ|é©æ–°|æ–°è¦|å…ˆé€²çš„)\b'
        ]
        
        terms = set()
        for pattern in technical_patterns:
            matches = re.findall(pattern, content)
            terms.update([term.lower() for term in matches])
        
        return sorted(list(terms))[:10]  # Return top 10 terms
    
    def _extract_innovation_keywords(self, content: str) -> List[str]:
        """Extract innovation-related keywords"""
        innovation_patterns = [
            r'(?i)\b(?:novel|new|innovative|breakthrough|advanced|cutting-edge|state-of-the-art|improved|enhanced|optimized|revolutionary|pioneering|groundbreaking)\b',
            r'(?i)\b(?:æ–°è¦|é©æ–°çš„|å…ˆé€²çš„|æœ€å…ˆç«¯|æ”¹è‰¯|æ‹¡å¼µ|æœ€é©åŒ–|é©å‘½çš„|å…ˆé§†çš„|ç”»æœŸçš„|ç‹¬å‰µçš„)\b'
        ]
        
        keywords = set()
        for pattern in innovation_patterns:
            matches = re.findall(pattern, content)
            keywords.update([kw.lower() for kw in matches])
        
        return sorted(list(keywords))[:5]
    
    def _extract_methodology_keywords(self, content: str) -> List[str]:
        """Extract methodology-related keywords"""
        method_patterns = [
            r'(?i)\b(?:method|approach|technique|algorithm|framework|model|system|architecture|design|implementation|evaluation|analysis|experiment|simulation|optimization|training|learning|inference|prediction|classification|clustering|detection|recognition)\b',
            r'(?i)\b(?:æ‰‹æ³•|ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ|æŠ€è¡“|ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ |ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯|ãƒ¢ãƒ‡ãƒ«|ã‚·ã‚¹ãƒ†ãƒ |ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£|è¨­è¨ˆ|å®Ÿè£…|è©•ä¾¡|åˆ†æž|å®Ÿé¨“|ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³|æœ€é©åŒ–|å­¦ç¿’|æŽ¨è«–|äºˆæ¸¬|åˆ†é¡ž|ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°|æ¤œå‡º|èªè­˜)\b'
        ]
        
        methods = set()
        for pattern in method_patterns:
            matches = re.findall(pattern, content)
            methods.update([method.lower() for method in matches])
        
        return sorted(list(methods))[:5]
    
    def _analyze_paper_section(self, paper_section: str) -> str:
        """Analyze individual paper section"""
        lines = paper_section.strip().split('\n')
        title_line = next((line for line in lines if line.startswith('è«–æ–‡')), '')
        abstract_line = next((line for line in lines if line.startswith('è¦æ—¨:')), '')
        
        # Extract key information
        technical_terms = self._extract_technical_terms(paper_section)[:3]
        innovations = self._extract_innovation_keywords(paper_section)[:2]
        
        analysis = f"- ä¸»è¦æŠ€è¡“: {', '.join(technical_terms) if technical_terms else 'ä¸€èˆ¬çš„ãªç ”ç©¶'}\n"
        analysis += f"- é©æ–°çš„è¦ç´ : {', '.join(innovations) if innovations else 'å¾“æ¥æ‰‹æ³•ã®æ”¹è‰¯'}\n"
        
        # Analyze abstract for key contributions
        if abstract_line:
            abstract_text = abstract_line.replace('è¦æ—¨:', '').strip()
            if len(abstract_text) > 50:
                analysis += f"- ä¸»è¦è²¢çŒ®: {abstract_text[:100]}...\n"
        
        return analysis
    
    def _generate_innovation_assessment(self, content: str) -> str:
        """Generate innovation assessment"""
        innovation_count = len(self._extract_innovation_keywords(content))
        technical_diversity = len(self._extract_technical_terms(content))
        
        if innovation_count > 5 and technical_diversity > 8:
            return "é«˜åº¦ãªé©æ–°æ€§ã‚’å«ã‚€ç ”ç©¶ç¾¤ã€‚è¤‡æ•°ã®æŠ€è¡“åˆ†é‡Žã§æ–°è¦æ€§ã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚"
        elif innovation_count > 2 and technical_diversity > 5:
            return "ä¸­ç¨‹åº¦ã®é©æ–°æ€§ã‚’æŒã¤ç ”ç©¶ç¾¤ã€‚æ—¢å­˜æŠ€è¡“ã®æ”¹è‰¯ã¨æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒçµ„ã¿åˆã‚ã•ã‚Œã¦ã„ã¾ã™ã€‚"
        else:
            return "å¾“æ¥ç ”ç©¶ã®æ”¹è‰¯ãƒ»æ‹¡å¼µãŒä¸­å¿ƒã€‚æ®µéšŽçš„ãªæŠ€è¡“ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚"
    
    def _generate_practical_assessment(self, content: str) -> str:
        """Generate practical application assessment"""
        practical_terms = len(re.findall(r'(?i)\b(?:application|implementation|practical|real-world|deployment|system|performance|efficiency|å®Ÿç”¨|å¿œç”¨|å®Ÿè£…|ã‚·ã‚¹ãƒ†ãƒ |æ€§èƒ½|åŠ¹çŽ‡)\b', content))
        
        if practical_terms > 10:
            return "å®Ÿç”¨åŒ–å¯èƒ½æ€§ãŒé«˜ã„ç ”ç©¶ç¾¤ã€‚ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚„æ€§èƒ½è©•ä¾¡ãŒé‡è¦–ã•ã‚Œã¦ã„ã¾ã™ã€‚"
        elif practical_terms > 5:
            return "å®Ÿç”¨åŒ–ã¸ã®é“ç­‹ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ç ”ç©¶ç¾¤ã€‚æ›´ãªã‚‹æ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚"
        else:
            return "åŸºç¤Žç ”ç©¶æ®µéšŽã®ç ”ç©¶ç¾¤ã€‚å®Ÿç”¨åŒ–ã«ã¯è¿½åŠ ã®é–‹ç™ºãŒå¿…è¦ã§ã™ã€‚"
    
    def _extract_technical_highlights(self, papers_data: List[Dict[str, Any]]) -> List[str]:
        """Extract technical highlights from papers"""
        highlights = []
        
        for paper in papers_data:
            title = paper.get('title', '')
            abstract = paper.get('abstract', '')
            
            # Look for performance metrics, innovations, etc.
            combined_text = f"{title} {abstract}"
            technical_terms = self._extract_technical_terms(combined_text)[:2]
            
            if technical_terms:
                highlights.append(f"{title}: {', '.join(technical_terms)}")
        
        return highlights[:5]  # Return top 5 highlights
    
    def _analyze_innovations(self, papers_data: List[Dict[str, Any]]) -> List[str]:
        """Analyze innovations across papers"""
        all_innovations = []
        
        for paper in papers_data:
            content = f"{paper.get('title', '')} {paper.get('abstract', '')} {paper.get('content', '')}"
            innovations = self._extract_innovation_keywords(content)
            if innovations:
                all_innovations.extend(innovations)
        
        # Count and return most common innovations
        from collections import Counter
        innovation_counts = Counter(all_innovations)
        return [f"{innovation} ({count}ä»¶)" for innovation, count in innovation_counts.most_common(5)]
    
    def _format_academic_summary(self, summary: str) -> str:
        """Format summary for academic presentation"""
        # Clean up and format the summary
        formatted = summary.strip()
        
        # Ensure proper Japanese formatting
        formatted = unicodedata.normalize('NFKC', formatted)
        
        # Add timestamp
        from datetime import datetime
        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
        
        formatted += f"\n\n---\nç”Ÿæˆæ—¥æ™‚: {timestamp}\nç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ : Academic LocalLLM"
        
        return formatted
    
    def is_available(self) -> bool:
        """Check if the academic LLM system is available"""
        return LLAMA_AVAILABLE or True  # Always available in template mode
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "llama_available": LLAMA_AVAILABLE,
            "model_initialized": self.is_initialized,
            "model_path": str(self.model_path) if self.model_path else None,
            "specialization": "academic-japanese-summarization",
            "fallback_mode": "enhanced-template" if not self.is_initialized else None
        }
