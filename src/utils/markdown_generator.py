"""
Markdown Report Generator for Real Llama Summaries
==================================================
"""

import json
import os
from datetime import datetime
from typing import Dict, Any
from pathlib import Path

class MarkdownReportGenerator:
    """Real Llamaè¦ç´„çµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    def __init__(self):
        self.output_dir = "results"
        
    def generate_summary_report(self, main_summary_file: str, individual_summaries_file: str = None) -> str:
        """Real Llamaè¦ç´„ã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        # Load main summary data
        with open(main_summary_file, 'r', encoding='utf-8') as f:
            main_data = json.load(f)
        
        # Load individual summaries if available
        individual_data = None
        if individual_summaries_file and os.path.exists(individual_summaries_file):
            with open(individual_summaries_file, 'r', encoding='utf-8') as f:
                individual_data = json.load(f)
        
        # Generate markdown content
        markdown_content = self._create_markdown_content(main_data, individual_data)
        
        # Save markdown file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"real_llama_summary_report_{timestamp}.md"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return filepath
    
    def _create_markdown_content(self, main_data: Dict[str, Any], individual_data: Dict[str, Any] = None) -> str:
        """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        
        # Header
        content = "# ğŸ¤– Real Llama AI å­¦è¡“è«–æ–‡è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ\n\n"
        content += f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n"
        content += "---\n\n"
        
        # Scan Information
        if 'scan_info' in main_data:
            scan_info = main_data['scan_info']
            content += "## ğŸ“Š ã‚¹ã‚­ãƒ£ãƒ³æƒ…å ±\n\n"
            content += f"- **å®Ÿè¡Œæ—¥æ™‚**: {scan_info.get('timestamp', 'ä¸æ˜')}\n"
            content += f"- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ•°**: {scan_info.get('total_sources', 0)}\n"
            content += f"- **ç·è«–æ–‡æ•°**: {scan_info.get('total_documents', 0)}\n\n"
        
        # Real Llama Summary
        if 'llm_summary' in main_data:
            content += "## ğŸ¯ Real Llama ç·åˆè¦ç´„\n\n"
            
            # Model Information
            if 'llm_summary_info' in main_data:
                summary_info = main_data['llm_summary_info']
                content += "### ğŸ§  LLMãƒ¢ãƒ‡ãƒ«æƒ…å ±\n\n"
                
                model_info = summary_info.get('model_info', {})
                content += f"- **ãƒ¢ãƒ‡ãƒ«å**: {model_info.get('model_name', 'ä¸æ˜')}\n"
                content += f"- **ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹**: `{model_info.get('model_path', 'ä¸æ˜')}`\n"
                content += f"- **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**: {model_info.get('backend', 'ä¸æ˜')}\n"
                content += f"- **å‡¦ç†æ–¹å¼**: {summary_info.get('processing_method', 'ä¸æ˜')}\n"
                content += f"- **å‡¦ç†æ™‚é–“**: {model_info.get('processing_time', 0):.1f}ç§’\n"
                content += f"- **ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°**: {model_info.get('tokens_generated', 0)}\n"
                content += f"- **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·**: {model_info.get('context_length', 0)}\n\n"
            
            # Summary Content
            content += "### ğŸ“„ è¦ç´„å†…å®¹\n\n"
            summary_text = main_data['llm_summary']
            content += f"{summary_text}\n\n"
        
        # Individual Summaries
        if individual_data and 'individual_summaries' in individual_data:
            content += "## ğŸ“š å€‹åˆ¥è«–æ–‡æ—¥æœ¬èªè¦ç´„ (Real Llamaç”Ÿæˆ)\n\n"
            
            # Processing Statistics
            content += "### ğŸ“ˆ å‡¦ç†çµ±è¨ˆ\n\n"
            content += f"- **å‡¦ç†è«–æ–‡æ•°**: {individual_data.get('total_papers', 0)}\n"
            content += f"- **ç·å‡¦ç†æ™‚é–“**: {individual_data.get('total_processing_time', 0):.1f}ç§’\n"
            content += f"- **å¹³å‡å‡¦ç†æ™‚é–“**: {individual_data.get('average_processing_time', 0):.1f}ç§’/è«–æ–‡\n\n"
            
            # Individual Papers
            summaries = individual_data['individual_summaries']
            for i, summary in enumerate(summaries):
                content += f"### ğŸ“ è«–æ–‡ {summary.get('paper_index', i+1)}\n\n"
                
                # Paper Information
                content += "#### ğŸ“‹ è«–æ–‡æƒ…å ±\n\n"
                content += f"- **ã‚¿ã‚¤ãƒˆãƒ«**: {summary.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')}\n"
                content += f"- **URL**: [{summary.get('url', '')}]({summary.get('url', '')})\n"
                content += f"- **ã‚½ãƒ¼ã‚¹**: {summary.get('source', 'ä¸æ˜')}\n"
                content += f"- **ã‚«ãƒ†ã‚´ãƒª**: {summary.get('category', 'ä¸æ˜')}\n"
                content += f"- **å‡¦ç†æ™‚é–“**: {summary.get('processing_time', 0):.1f}ç§’\n"
                content += f"- **è¦ç´„æ–‡å­—æ•°**: {summary.get('summary_length', 0)}æ–‡å­—\n\n"
                
                # Original Abstract
                original_abstract = summary.get('original_abstract', '')
                if original_abstract:
                    content += "#### ğŸ“„ åŸæ–‡æ¦‚è¦\n\n"
                    content += f"```\n{original_abstract}\n```\n\n"
                
                # Japanese Summary
                content += "#### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªè¦ç´„ (Real Llamaç”Ÿæˆ)\n\n"
                japanese_summary = summary.get('japanese_summary', '')
                content += f"{japanese_summary}\n\n"
                
                content += "---\n\n"
        
        # Source Details
        if 'sources' in main_data:
            content += "## ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è©³ç´°\n\n"
            
            for source_name, source_data in main_data['sources'].items():
                content += f"### ğŸ“Š {source_name.upper()}\n\n"
                content += f"- **æ¤œç´¢URL**: {source_data.get('search_url', 'ä¸æ˜')}\n"
                content += f"- **æ–‡æ›¸æ•°**: {source_data.get('document_count', 0)}\n\n"
                
                if 'documents' in source_data and len(source_data['documents']) > 0:
                    content += "#### ğŸ“„ åé›†è«–æ–‡ä¸€è¦§\n\n"
                    for i, doc in enumerate(source_data['documents'][:10]):  # Show first 10
                        content += f"{i+1}. **{doc.get('name', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')}**\n"
                        content += f"   - URL: [{doc.get('url', '')}]({doc.get('url', '')})\n"
                        content += f"   - ã‚«ãƒ†ã‚´ãƒª: {doc.get('category', 'ä¸æ˜')}\n\n"
                    
                    if len(source_data['documents']) > 10:
                        content += f"   *(ä»– {len(source_data['documents']) - 10} ä»¶)*\n\n"
        
        # Footer
        content += "---\n\n"
        content += "## ğŸ”§ æŠ€è¡“æƒ…å ±\n\n"
        content += "- **ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ **: InfoGatherer with Real Llama\n"
        content += "- **LLMã‚¨ãƒ³ã‚¸ãƒ³**: llama-cpp-python\n"
        content += "- **å‡¦ç†ã‚¿ã‚¤ãƒ—**: ãƒ­ãƒ¼ã‚«ãƒ«LLM (ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·)\n"
        content += "- **å‡ºåŠ›å½¢å¼**: Markdown Report\n\n"
        content += f"*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {datetime.now().isoformat()}*\n"
        
        return content
