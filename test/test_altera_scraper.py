#!/usr/bin/env python3
"""
æ”¹å–„ã•ã‚ŒãŸAlteraã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import yaml
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.scrapers.altera_scraper import AlteraScraper


def test_altera_scraper():
    """æ”¹å–„ã•ã‚ŒãŸAlteraã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("æ”¹å–„ã•ã‚ŒãŸAlteraã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        config_path = os.path.join(project_root, 'config', 'settings.yaml')
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        altera_config = config['data_sources']['altera']
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
        scraper = AlteraScraper(altera_config)
        
        # URLæ§‹ç¯‰ã‚’ãƒ†ã‚¹ãƒˆ
        search_url = scraper._build_search_url()
        print(f"ğŸ”— æ§‹ç¯‰ã•ã‚ŒãŸURL: {search_url}")
        
        # è¨­å®šå€¤ã‚’ç¢ºèª
        print(f"ğŸ“Š è¨­å®šå€¤:")
        print(f"  max_results: {altera_config.get('max_results', 100)}")
        print(f"  scroll_pages: {altera_config.get('scroll_pages', 5)}")
        print(f"  query: {altera_config.get('search_params', {}).get('query', 'Unknown')}")
        
        # URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
        if 'q=DSP' in search_url:
            print("  âœ… DSPã‚¯ã‚¨ãƒªãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("  âŒ DSPã‚¯ã‚¨ãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        if 's=Relevancy' in search_url:
            print("  âœ… ã‚½ãƒ¼ãƒˆè¨­å®šãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("  âŒ ã‚½ãƒ¼ãƒˆè¨­å®šãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print("\nğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        documents = scraper.scrape_documents()
        
        print(f"âœ… å–å¾—å®Œäº†: {len(documents)} ä»¶")
        
        if documents:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            urls = [doc.url for doc in documents]
            unique_urls = set(urls)
            duplicate_count = len(urls) - len(unique_urls)
            
            print(f"ğŸ“ˆ é‡è¤‡åˆ†æ:")
            print(f"  ç·ä»¶æ•°: {len(urls)}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯: {len(unique_urls)}")
            print(f"  é‡è¤‡: {duplicate_count}")
            
            if duplicate_count > 0:
                print(f"  é‡è¤‡ç‡: {duplicate_count / len(urls) * 100:.1f}%")
            else:
                print("  é‡è¤‡ç‡: 0% (å®Œç’§ï¼)")
            
            # æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®ç¢ºèª
            expected_docs = [
                'dsp builder',
                'nios',
                'stratix',
                'user guide',
                'handbook',
                'reference manual',
                'ip core'
            ]
            
            found_expected = {}
            for expected in expected_docs:
                count = sum(1 for doc in documents if expected.lower() in doc.name.lower())
                found_expected[expected] = count
            
            print(f"\nğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—:")
            for doc_type, count in found_expected.items():
                status = "âœ…" if count > 0 else "âŒ"
                print(f"  {status} {doc_type}: {count} ä»¶")
            
            # DSP BuilderãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            dsp_builder_docs = [doc for doc in documents if 'dsp builder' in doc.name.lower()]
            if dsp_builder_docs:
                print(f"\nğŸ¯ DSP Builderé–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(dsp_builder_docs)} ä»¶")
                for doc in dsp_builder_docs[:3]:
                    print(f"  - {doc.name}")
            
            # Niosé–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒã‚§ãƒƒã‚¯
            nios_docs = [doc for doc in documents if 'nios' in doc.name.lower()]
            if nios_docs:
                print(f"\nğŸ¯ Niosé–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(nios_docs)} ä»¶")
                for doc in nios_docs[:3]:
                    print(f"  - {doc.name}")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
            categories = {}
            fpga_series = {}
            
            for doc in documents:
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
                cat = doc.category or 'Unknown'
                categories[cat] = categories.get(cat, 0) + 1
                
                # FPGAã‚·ãƒªãƒ¼ã‚ºåˆ¥é›†è¨ˆ
                series = doc.fpga_series or 'Unknown'
                fpga_series[series] = fpga_series.get(series, 0) + 1
            
            print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ:")
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"  {category}: {count}")
            
            print(f"\nğŸ”§ FPGAã‚·ãƒªãƒ¼ã‚ºåˆ¥é›†è¨ˆ:")
            for series, count in sorted(fpga_series.items(), key=lambda x: x[1], reverse=True):
                print(f"  {series}: {count}")
            
            # æ¤œç´¢URLã®ç¢ºèª
            search_url_from_doc = documents[0].search_url
            print(f"\nğŸ”— å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸæ¤œç´¢URL:")
            print(f"  {search_url_from_doc}")
            
            # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
            print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«çµæœ (æœ€åˆã®10ä»¶):")
            for i, doc in enumerate(documents[:10]):
                print(f"  {i+1}. {doc.name}")
                print(f"     URL: {doc.url}")
                print(f"     ã‚«ãƒ†ã‚´ãƒª: {doc.category}")
                print(f"     FPGAã‚·ãƒªãƒ¼ã‚º: {doc.fpga_series}")
                print()
            
            # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            results_file = os.path.join(project_root, 'results', 'altera_test_results.json')
            os.makedirs(os.path.dirname(results_file), exist_ok=True)
            
            # çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            serializable_results = {
                'timestamp': datetime.now().isoformat(),
                'search_url': search_url,
                'total_documents': len(documents),
                'unique_documents': len(unique_urls),
                'duplicate_count': duplicate_count,
                'categories': categories,
                'fpga_series': fpga_series,
                'documents': [
                    {
                        'name': doc.name,
                        'url': str(doc.url),
                        'category': doc.category,
                        'fpga_series': doc.fpga_series,
                        'file_type': doc.file_type,
                        'search_url': str(doc.search_url) if doc.search_url else None
                    }
                    for doc in documents
                ]
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_altera_scraper()
    
    if success:
        print("\nğŸ‰ Alteraã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    else:
        print("\nğŸ’¥ ãƒ†ã‚¹ãƒˆå¤±æ•—")
        sys.exit(1)