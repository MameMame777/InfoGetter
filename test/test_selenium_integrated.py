#!/usr/bin/env python3
"""
Seleniumå°‚ç”¨çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.main import InfoGatherer
from utils.log_manager import clear_log_file


def test_selenium_only_scrapers():
    """Seleniumå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("=" * 80)
    print("Seleniumå°‚ç”¨FPGAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–
    print("ğŸ—‚ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
    clear_log_file()
    
    try:
        # InfoGathererã‚’åˆæœŸåŒ–
        config_path = os.path.join(project_root, 'config', 'settings.yaml')
        gatherer = InfoGatherer(config_path)
        
        # ä¸¡æ–¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’å®Ÿè¡Œ
        print("\nğŸ” ä¸¡æ–¹ã®Seleniumã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
        print("â³ ãƒ–ãƒ©ã‚¦ã‚¶ã®èµ·å‹•ã¨ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚’å¾…æ©Ÿä¸­...")
        
        results = gatherer.run_scraping(sources=['xilinx', 'altera'])
        
        print(f"\nğŸ“Š ç·åˆçµæœåˆ†æ:")
        
        total_documents = 0
        total_unique_urls = set()
        
        for source, documents in results.items():
            print(f"\nğŸ”§ {source.upper()} çµæœ:")
            print(f"  å–å¾—ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
            
            if documents:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                urls = [doc.url for doc in documents]
                unique_urls = set(urls)
                duplicate_count = len(urls) - len(unique_urls)
                
                total_documents += len(documents)
                total_unique_urls.update(unique_urls)
                
                print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(unique_urls)}")
                print(f"  é‡è¤‡æ•°: {duplicate_count}")
                
                if duplicate_count > 0:
                    print(f"  é‡è¤‡ç‡: {duplicate_count / len(urls) * 100:.1f}%")
                else:
                    print("  é‡è¤‡ç‡: 0% (å®Œç’§ï¼)")
                
                # æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç¢ºèª
                expected_docs = {
                    'xilinx': ['system generator', 'fir compiler', 'fft', 'dsp', 'versal', 'zynq'],
                    'altera': ['dsp builder', 'nios', 'stratix', 'variable precision', 'floating point']
                }
                
                source_expected = expected_docs.get(source, [])
                found_expected = {}
                for expected in source_expected:
                    count = sum(1 for doc in documents if expected.lower() in doc.name.lower())
                    found_expected[expected] = count
                
                print(f"  æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
                total_expected = 0
                for doc_type, count in found_expected.items():
                    status = "âœ…" if count > 0 else "âŒ"
                    total_expected += count
                    print(f"    {status} {doc_type}: {count} ä»¶")
                
                print(f"  æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç·æ•°: {total_expected} ä»¶")
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
                categories = {}
                fpga_series = {}
                
                for doc in documents:
                    cat = doc.category or 'Unknown'
                    categories[cat] = categories.get(cat, 0) + 1
                    
                    series = doc.fpga_series or 'Unknown'
                    fpga_series[series] = fpga_series.get(series, 0) + 1
                
                print(f"  ä¸»è¦ã‚«ãƒ†ã‚´ãƒª: {dict(list(sorted(categories.items(), key=lambda x: x[1], reverse=True))[:3])}")
                print(f"  ä¸»è¦FPGAã‚·ãƒªãƒ¼ã‚º: {dict(list(sorted(fpga_series.items(), key=lambda x: x[1], reverse=True))[:3])}")
                
                # æ¤œç´¢URLã®ç¢ºèª
                search_url = documents[0].search_url
                print(f"  æ¤œç´¢URL: {search_url}")
                
                # DSPã‚¯ã‚¨ãƒªã®ç¢ºèª
                if source == 'xilinx':
                    if 'query=DSP' in search_url:
                        print("  âœ… Xilinx DSPã‚¯ã‚¨ãƒªãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
                    else:
                        print("  âŒ Xilinx DSPã‚¯ã‚¨ãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                elif source == 'altera':
                    if 'q=DSP' in search_url:
                        print("  âœ… Altera DSPã‚¯ã‚¨ãƒªãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
                    else:
                        print("  âŒ Altera DSPã‚¯ã‚¨ãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"\nğŸ¯ ç·åˆçµ±è¨ˆ:")
        print(f"  ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_documents}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(total_unique_urls)}")
        print(f"  ã‚½ãƒ¼ã‚¹é–“é‡è¤‡: {total_documents - len(total_unique_urls)}")
        
        # å“è³ªè©•ä¾¡
        quality_score = 0
        max_score = 100
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã§ã®è©•ä¾¡ (40ç‚¹æº€ç‚¹)
        if total_documents >= 50:
            quality_score += 40
        elif total_documents >= 30:
            quality_score += 30
        elif total_documents >= 10:
            quality_score += 20
        
        # é‡è¤‡ç‡ã§ã®è©•ä¾¡ (30ç‚¹æº€ç‚¹)
        if total_documents > 0:
            duplicate_rate = (total_documents - len(total_unique_urls)) / total_documents
            if duplicate_rate <= 0.1:  # 10%ä»¥ä¸‹
                quality_score += 30
            elif duplicate_rate <= 0.2:  # 20%ä»¥ä¸‹
                quality_score += 20
            elif duplicate_rate <= 0.3:  # 30%ä»¥ä¸‹
                quality_score += 10
        
        # æœŸå¾…ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç™ºè¦‹ç‡ (30ç‚¹æº€ç‚¹)
        expected_found = 0
        total_expected = 0
        for source, documents in results.items():
            if documents:
                expected_docs = {
                    'xilinx': ['system generator', 'fir compiler', 'fft', 'dsp'],
                    'altera': ['dsp builder', 'nios', 'stratix', 'variable precision']
                }
                source_expected = expected_docs.get(source, [])
                for expected in source_expected:
                    total_expected += 1
                    if any(expected.lower() in doc.name.lower() for doc in documents):
                        expected_found += 1
        
        if total_expected > 0:
            expected_rate = expected_found / total_expected
            quality_score += int(expected_rate * 30)
        
        print(f"\nğŸ“ˆ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å“è³ªã‚¹ã‚³ã‚¢: {quality_score}/{max_score} ç‚¹")
        
        if quality_score >= 80:
            print("ğŸ‰ å„ªç§€: é«˜å“è³ªãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™")
        elif quality_score >= 60:
            print("ğŸ‘ è‰¯å¥½: è‰¯è³ªãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™")
        elif quality_score >= 40:
            print("âš ï¸  æ™®é€š: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å“è³ªã«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        else:
            print("âŒ è¦æ”¹å–„: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å“è³ªãŒä½ã„ã§ã™")
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        results_file = os.path.join(project_root, 'results', 'selenium_integrated_test_results.json')
        os.makedirs(os.path.dirname(results_file), exist_ok=True)
        
        # çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        serializable_results = {
            'timestamp': datetime.now().isoformat(),
            'total_documents': total_documents,
            'total_unique_urls': len(total_unique_urls),
            'quality_score': quality_score,
            'expected_found': expected_found,
            'total_expected': total_expected,
            'sources': {}
        }
        
        for source, documents in results.items():
            serializable_results['sources'][source] = {
                'document_count': len(documents),
                'search_url': documents[0].search_url if documents else None,
                'documents': [
                    {
                        'name': doc.name,
                        'url': str(doc.url),
                        'category': doc.category,
                        'fpga_series': doc.fpga_series,
                        'file_type': doc.file_type,
                        'search_url': str(doc.search_url) if doc.search_url else None
                    }
                    for doc in documents
                ]
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çµ±åˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_selenium_only_scrapers()
    
    if success:
        print("\nğŸ‰ Seleniumå°‚ç”¨çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    else:
        print("\nğŸ’¥ ãƒ†ã‚¹ãƒˆå¤±æ•—")
        sys.exit(1)