#!/usr/bin/env python3
"""
æ”¹å–„ã•ã‚ŒãŸXilinxã¨Alteraã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.main import InfoGatherer


def test_improved_scrapers():
    """æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("=" * 80)
    print("æ”¹å–„ã•ã‚ŒãŸFPGAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    try:
        # InfoGathererã‚’åˆæœŸåŒ–
        config_path = os.path.join(project_root, 'config', 'settings.yaml')
        gatherer = InfoGatherer(config_path)
        
        # ä¸¡æ–¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’å®Ÿè¡Œ
        print("\nğŸ” ä¸¡æ–¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹...")
        results = gatherer.run_scraping(sources=['xilinx', 'altera'])
        
        print(f"\nğŸ“Š ç·åˆçµæœåˆ†æ:")
        
        total_documents = 0
        total_unique_urls = set()
        
        for source, documents in results.items():
            print(f"\nğŸ”§ {source.upper()} çµæœ:")
            print(f"  å–å¾—ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
            
            if documents:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                urls = [doc.url for doc in documents]
                unique_urls = set(urls)
                duplicate_count = len(urls) - len(unique_urls)
                
                total_documents += len(documents)
                total_unique_urls.update(unique_urls)
                
                print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(unique_urls)}")
                print(f"  é‡è¤‡æ•°: {duplicate_count}")
                
                if duplicate_count > 0:
                    print(f"  é‡è¤‡ç‡: {duplicate_count / len(urls) * 100:.1f}%")
                else:
                    print("  é‡è¤‡ç‡: 0% (å®Œç’§ï¼)")
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
                categories = {}
                fpga_series = {}
                
                for doc in documents:
                    cat = doc.category or 'Unknown'
                    categories[cat] = categories.get(cat, 0) + 1
                    
                    series = doc.fpga_series or 'Unknown'
                    fpga_series[series] = fpga_series.get(series, 0) + 1
                
                print(f"  ä¸»è¦ã‚«ãƒ†ã‚´ãƒª: {dict(list(sorted(categories.items(), key=lambda x: x[1], reverse=True))[:3])}")
                print(f"  ä¸»è¦FPGAã‚·ãƒªãƒ¼ã‚º: {dict(list(sorted(fpga_series.items(), key=lambda x: x[1], reverse=True))[:3])}")
                
                # æ¤œç´¢URLã®ç¢ºèª
                search_url = documents[0].search_url
                print(f"  æ¤œç´¢URL: {search_url}")
                
                # DSPã‚¯ã‚¨ãƒªã®ç¢ºèª
                if source == 'xilinx':
                    if 'query=DSP' in search_url:
                        print("  âœ… Xilinx DSPã‚¯ã‚¨ãƒªãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
                    else:
                        print("  âŒ Xilinx DSPã‚¯ã‚¨ãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                elif source == 'altera':
                    if 'q=DSP' in search_url:
                        print("  âœ… Altera DSPã‚¯ã‚¨ãƒªãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
                    else:
                        print("  âŒ Altera DSPã‚¯ã‚¨ãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"\nğŸ¯ ç·åˆçµ±è¨ˆ:")
        print(f"  ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_documents}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(total_unique_urls)}")
        print(f"  ã‚½ãƒ¼ã‚¹é–“é‡è¤‡: {total_documents - len(total_unique_urls)}")
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        results_file = os.path.join(project_root, 'results', 'integrated_test_results.json')
        os.makedirs(os.path.dirname(results_file), exist_ok=True)
        
        # çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        serializable_results = {
            'timestamp': datetime.now().isoformat(),
            'total_documents': total_documents,
            'total_unique_urls': len(total_unique_urls),
            'sources': {}
        }
        
        for source, documents in results.items():
            serializable_results['sources'][source] = {
                'document_count': len(documents),
                'search_url': documents[0].search_url if documents else None,
                'documents': [
                    {
                        'name': doc.name,
                        'url': str(doc.url),
                        'category': doc.category,
                        'fpga_series': doc.fpga_series,
                        'file_type': doc.file_type,
                        'search_url': str(doc.search_url) if doc.search_url else None
                    }
                    for doc in documents
                ]
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çµ±åˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_improved_scrapers()
    
    if success:
        print("\nğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    else:
        print("\nğŸ’¥ ãƒ†ã‚¹ãƒˆå¤±æ•—")
        sys.exit(1)